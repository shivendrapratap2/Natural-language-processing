{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertoX.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWSaYKVbKJfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Info\n",
        "# Challange Name : Jigsaw Multilingual Toxic Comment Classification\n",
        "# Link : https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification\n",
        "\n",
        "# Proceeding >>>\n",
        "# i.   used pretrained BERT (Bidirectional Encoder Representations for Transformers) for toxic text classification task.\n",
        "# ii.  Finetuned model with additional classification layer for 2 epochs with adam optimizer with lr = .00005, batch_size = 32.\n",
        "# iii. training accuracy = 95.50%, validation accuracy = 84.69%\n",
        "\n",
        "# Dependencies\n",
        "# i.   tensorflow version 1.x, tensorflow_hub, bert-tensorflow\n",
        "# ii.  keras, numpy, pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YdodADoPwn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Dropout, Dense, Embedding, Input, LSTM, Bidirectional, GlobalAveragePooling2D, Layer\n",
        "from keras import optimizers\n",
        "from keras.activations import sigmoid\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veXoa1drlAxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUEjnMlvsyj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.output_size = 768\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module('https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1', trainable=True)\n",
        "\n",
        "        trainable_vars = self.bert.variables       \n",
        "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "        \n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\"pooled_output\"]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], self.output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZKDyfwiPsXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Metadata\n",
        "train_file = '/content/drive/My Drive/Project_BertoX/train-processed-seqlen128.csv'\n",
        "validation_file = '/content/drive/My Drive/Project_BertoX/validation-processed-seqlen128.csv'\n",
        "batch_size = 32\n",
        "train_len = 223549 # len(pd.read_csv(train_file))\n",
        "valid_len = 8000   # len(pd.read_csv(validation_file))\n",
        "learning_rate = 0.00005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc7la1CBt8-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Generator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def DataGen(filename, batch = 32):\n",
        "\n",
        "  df = pd.read_csv(filename)\n",
        "  df = df[['input_word_ids','input_mask', 'all_segment_id', 'toxic']]\n",
        "  count = 0\n",
        "  inp_ids = []\n",
        "  inp_masks = []\n",
        "  seg_ids = []\n",
        "  while (True):\n",
        "\n",
        "    cols = df.columns\n",
        "    inp_id = df[cols[0]][count:count+batch].tolist()\n",
        "    inp_mask = df[cols[1]][count:count+batch].tolist()\n",
        "    seg_id = df[cols[2]][count:count+batch].tolist()\n",
        "    labels = df[cols[3]][count:count+batch].tolist()\n",
        "\n",
        "    # converting string tuple into integer list\n",
        "    for i in range(batch):\n",
        "      inp_ids.append([int(x) for x in inp_id[i][1:-1].split(', ')])\n",
        "      inp_masks.append([int(x) for x in inp_mask[i][1:-1].split(', ')])\n",
        "      seg_ids.append([int(x) for x in seg_id[i][1:-1].split(', ')])\n",
        "\n",
        "    count+=batch\n",
        "    if(count+batch>=len(df)):\n",
        "      count=0\n",
        "\n",
        "    yield ([np.array(inp_ids), np.array(inp_masks), np.array(seg_ids)], np.array(labels))\n",
        "    inp_ids = []\n",
        "    inp_masks = []\n",
        "    seg_ids = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HA-_fkMMz9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = DataGen(filename=train_file, batch = batch_size)\n",
        "val_gen = DataGen(filename=validation_file, batch = batch_size)\n",
        "x = train_gen.__next__()\n",
        "print(x[0][0].shape,x[0][1].shape,x[0][2].shape, x[1].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxlmmPKvtDjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Bertox(Inp_shape):\n",
        "\n",
        "    in_id = Input(shape=Inp_shape)\n",
        "    in_mask = Input(shape=Inp_shape)\n",
        "    in_segment = Input(shape=Inp_shape)\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "\n",
        "    bert_output = BertLayer()(bert_inputs)\n",
        "    do = Dropout(0.1)(bert_output)\n",
        "    pred = Dense(1, activation='sigmoid')(do)\n",
        "\n",
        "    model = Model(inputs=bert_inputs, outputs=pred)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYrPk7J_xC4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Bertox((128,))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID_0HIWHFRq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam, metrics=['accuracy'])\n",
        "train_record = model.fit_generator(train_gen, steps_per_epoch=train_len/batch_size, validation_data= val_gen,\n",
        "                                   validation_steps=valid_len/batch_size, epochs=2, shuffle=True, initial_epoch=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTsGlMU5rgww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/BertoX.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwL0aCuYrkCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing\n",
        "df = pd.read_csv('/content/drive/My Drive/Project_BertoX/test-processed-seqlen128.csv')\n",
        "df = df[['input_word_ids','input_mask', 'all_segment_id']]\n",
        "cols = df.columns\n",
        "inp_ids = []\n",
        "inp_masks = []\n",
        "seg_ids = []\n",
        "\n",
        "cols = df.columns\n",
        "inp_id = df[cols[0]].tolist()\n",
        "inp_mask = df[cols[1]].tolist()\n",
        "seg_id = df[cols[2]].tolist()\n",
        "\n",
        "for i in range(len(df)):\n",
        "  inp_ids.append([int(x) for x in inp_id[i][1:-1].split(', ')])\n",
        "  inp_masks.append([int(x) for x in inp_mask[i][1:-1].split(', ')])\n",
        "  seg_ids.append([int(x) for x in seg_id[i][1:-1].split(', ')])\n",
        "print(np.array(inp_ids).shape, np.array(inp_masks).shape, np.array(seg_ids).shape)\n",
        "\n",
        "prediction = model.predict([np.array(inp_ids), np.array(inp_masks), np.array(seg_ids)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLofDvRLvsdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_frame = pd.DataFrame(prediction)\n",
        "pred_frame.to_csv('predict.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
# -*- coding: utf-8 -*-
"""LangGen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y83wnSG-9Kx7_Le7YwvWCb-AKUIg_G0I
"""

from keras.layers import Layer
import keras.backend as K
from keras.models import Model,load_model
from keras.optimizers import RMSprop
from keras import layers
import numpy as np
from collections import defaultdict
import os, string
import random
from random import shuffle
from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.layers.embeddings import Embedding
from keras.layers import Input, Dense, LSTM

import nltk
nltk.download('all')
#################@@ text file prep code @@#######################

def ReadandClean(filename):
  
  with open(filename, 'r') as f:
    data = f.read()
  cleanedCorpus = dict()  
  data = data.split('\n')
  print('No. keywords-sentence pairs = ', len(data))
  table = str.maketrans('', '', string.punctuation)
  stopwords = ['a','an','the']
  for line in data:
    key, desc = line.split(':')
    key = key.split(',')
    key = [word.lower() for word in key]
    key = [w.translate(table) for w in key]
    key = [word for word in key if word.isalpha()]
    key = [word for word in key if word not in stopwords]
    desc = desc.strip()
    desc = desc.split(' ')
    desc = [word.lower() for word in desc]
    desc = [w.translate(table) for w in desc]
    desc = [word for word in desc if word.isalpha()]
    desc = [word for word in desc if word not in stopwords]
    cleanedCorpus[' '.join(key)] = desc
    
  return cleanedCorpus
    
def vocabulary(corpus):
  
  vocab = list()
  for key in corpus:
    desc = corpus[key]
    vocab = vocab + desc
  return set(vocab)

def All_sentences(corpus):
  sentences = list()
  for key in corpus:
    sentences.append(corpus[key])
  return sentences


def oneHot(line,token_index,mode):
  idxlist = []
  if mode == 'key':
    for word in line.split():
      idxlist.append(token_index[word])
  else:
    for word in line.split()[:-1]:
      idxlist.append(token_index[word])
    
  return idxlist

def train_data(token_index):  
  maxLen = len(max(sentences, key = len))
  encoded_keys = [oneHot(key, token_index,'key') for key in corpus]
  padded_keys = pad_sequences(encoded_keys, maxlen=5, padding='post')
  encoded_desc = [oneHot(' '.join(corpus[key]), token_index,'desc') for key in corpus]
  padded_desc = pad_sequences(encoded_desc, maxlen=maxLen, padding='post')
  encoder_x = np.array(padded_keys)
  decoder_x = np.array(padded_desc)
  decoder_y = np.zeros((len(corpus),maxLen,vocab_size+1))
  r,c,_ = decoder_y.shape
  for i in range(r):
    for j in range(1,c):
      if decoder_x[i][j] != 0:
        decoder_y[i][j-1][decoder_x[i][j]] = 1

  return encoder_x, decoder_x, decoder_y

def decode_sequence(input_seq, token_index, reverse_token_index):
    states_value = encoder_model.predict(input_seq)    
    target_seq = np.zeros((1,1))
    
    target_seq[0, 0] = token_index['sos']
    
    stop_condition = False
    decoded_sentence = ''
    
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        #print(sampled_token_index)
        sampled_char = reverse_token_index[sampled_token_index]
        decoded_sentence += ' '+sampled_char
        #print(sampled_char)
        if (sampled_char == 'eos' or len(decoded_sentence.split()) >= 10):
            stop_condition = True
        
        target_seq = np.zeros((1,1))
        target_seq[0, 0] = sampled_token_index
        
        states_value = [h, c]
    
    return decoded_sentence
  
############################ Data Preprocessing ########################
with open('Flickr8k.token.txt', 'r') as f:
  data = f.read()
  
data = data.split('\n')
minLenData = list()
for i in range(0,len(data),4):
  minLenData.append(min(data[i:i+4], key = len))

corpora = list()
for desc in minLenData:
  if len(desc.split()) > 0:
    if desc.split()[-1] == '.':
      desc = desc.split()[1:-1]
    else:
      desc = desc.split()[1:]
    corpora.append('sos '+' '.join(desc)+' eos')
corpora = '\n'.join(corpora)

with open('Mincorpus.txt', 'w') as f:
  f.write(corpora)

with open('Mincorpus.txt', 'r') as f:
        data = f.read()
data = data.split('\n')

trainData = []
helpingVerb = ['is','am','are','was','were','will','would','shall', 'should']

for sentence in data:
  word = nltk.word_tokenize(sentence)
  tagged = nltk.pos_tag(word[1:-1])
  keys = list()
  count = 0
  for word, tag in tagged:
    if tag[0] == 'N' and count<5:
      keys.append(word)
      count+=1

    if tag[0] == 'V' and word not in helpingVerb and count<5:
      keys.append(word)
      count+=1

    if tag[0] == 'J' and count<5:
      keys.append(word)
      count+=1

    if tag[0] == 'R' and count<5:
      keys.append(word)
      count+=1

  trainData.append(','.join(keys)+ ': '+ sentence)

with open('Train.txt', 'w') as fp:
  fp.write('\n'.join(trainData))
    
filename = 'Train.txt'
corpus = ReadandClean(filename)
vocab = sorted(list(vocabulary(corpus)))
sentences = All_sentences(corpus)
vocab_size = len(vocab)
maxLen = len(max(sentences, key = len))
token_index = dict([(word, i+1) for i, word in enumerate(vocab)])
reverse_token_index = dict((i, word) for word, i in token_index.items())
print('Vocab size = ', len(vocab))

################## Model Loading ##########################

model_1 = load_model('LanGen.h5')
weights2set = dict()
for layer in model_1.layers:
  weights2set[layer.name] = layer.get_weights()

#################### training inference setup ###############
encoder_inputs = Input(shape=(None,))

enc_emb_layer = Embedding(vocab_size+1, 50, mask_zero = True, weights=weights2set['embedding_1'])
x = enc_emb_layer(encoder_inputs)
enc_lstm_layer = LSTM(50,return_state=True, weights=weights2set['lstm_1'])
x, state_h, state_c = enc_lstm_layer(x)
encoder_states = [state_h, state_c]

decoder_inputs = Input(shape=(None,))

dec_emb_layer = Embedding(vocab_size+1, 50, mask_zero = True, weights=weights2set['embedding_2'])
x = dec_emb_layer(decoder_inputs)
dec_lstm_layer = LSTM(50, return_sequences=True, return_state= True, weights=weights2set['lstm_2'])
x, _, _ = dec_lstm_layer(x, initial_state=encoder_states)
dec_dense =  Dense(vocab_size+1, activation='softmax', weights=weights2set['dense_1'])
decoder_outputs = dec_dense(x)
model_2 = Model([encoder_inputs, decoder_inputs], decoder_outputs)


################# testing inference setup ###################

encoder_model = Model(encoder_inputs, encoder_states)

decoder_state_input_h = Input(shape=(50,))
decoder_state_input_c = Input(shape=(50,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

dec_emb2= dec_emb_layer(decoder_inputs)
decoder_outputs2, state_h2, state_c2 = dec_lstm_layer(dec_emb2, initial_state=decoder_states_inputs)
decoder_states2 = [state_h2, state_c2]

decoder_outputs2 = dec_dense(decoder_outputs2)

decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs2] + decoder_states2)



############################### testing #################################

# ex = ['dog running beach', 'basketball game', 'woman yellow jogging', 'couple pose wedding day']  
inp_str = input('Enter space seperated keywords: ')
#for inp_str in inp_list:
inp_seq = [oneHot(inp_str, token_index, 'key')]
out_seq = decode_sequence(inp_seq,token_index, reverse_token_index)
print('out_seq :', out_seq)


